{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "(23428, 1, 47)\n",
      "(23428,)\n",
      "building model\n",
      "compiling model\n",
      "Train on 23428 samples, validate on 2929 samples\n",
      "Epoch 1/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0576 - mean_squared_error: 0.0575Epoch 00000: val_loss improved from inf to 0.05518, saving model to bestmodel.hdf5\n",
      "23428/23428 [==============================] - 2s - loss: 0.0575 - mean_squared_error: 0.0574 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 2/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0537 - mean_squared_error: 0.0536Epoch 00001: val_loss improved from 0.05518 to 0.05509, saving model to bestmodel.hdf5\n",
      "23428/23428 [==============================] - 2s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 3/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0535 - mean_squared_error: 0.0534Epoch 00002: val_loss improved from 0.05509 to 0.05495, saving model to bestmodel.hdf5\n",
      "23428/23428 [==============================] - 2s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 4/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00003: val_loss improved from 0.05495 to 0.05474, saving model to bestmodel.hdf5\n",
      "23428/23428 [==============================] - 2s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 5/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0537 - mean_squared_error: 0.0536Epoch 00004: val_loss did not improve\n",
      "23428/23428 [==============================] - 2s - loss: 0.0537 - mean_squared_error: 0.0536 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 6/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00005: val_loss did not improve\n",
      "23428/23428 [==============================] - 3s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 7/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00006: val_loss did not improve\n",
      "23428/23428 [==============================] - 3s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 8/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0537 - mean_squared_error: 0.0536Epoch 00007: val_loss did not improve\n",
      "23428/23428 [==============================] - 3s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 9/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0537 - mean_squared_error: 0.0536Epoch 00008: val_loss did not improve\n",
      "23428/23428 [==============================] - 2s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 10/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0538 - mean_squared_error: 0.0537Epoch 00009: val_loss did not improve\n",
      "23428/23428 [==============================] - 2s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 11/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00010: val_loss did not improve\n",
      "23428/23428 [==============================] - 2s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 12/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00011: val_loss did not improve\n",
      "23428/23428 [==============================] - 2s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 13/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00012: val_loss improved from 0.05474 to 0.05472, saving model to bestmodel.hdf5\n",
      "23428/23428 [==============================] - 2s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 14/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0537 - mean_squared_error: 0.0535Epoch 00013: val_loss did not improve\n",
      "23428/23428 [==============================] - 3s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 15/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00014: val_loss did not improve\n",
      "23428/23428 [==============================] - 4s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 16/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00015: val_loss did not improve\n",
      "23428/23428 [==============================] - 8s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 17/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0535 - mean_squared_error: 0.0534Epoch 00016: val_loss did not improve\n",
      "23428/23428 [==============================] - 11s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 18/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0535 - mean_squared_error: 0.0534Epoch 00017: val_loss did not improve\n",
      "23428/23428 [==============================] - 12s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 19/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00018: val_loss did not improve\n",
      "23428/23428 [==============================] - 12s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 20/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00019: val_loss did not improve\n",
      "23428/23428 [==============================] - 12s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 21/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00020: val_loss did not improve\n",
      "23428/23428 [==============================] - 12s - loss: 0.0537 - mean_squared_error: 0.0536 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 22/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00021: val_loss did not improve\n",
      "23428/23428 [==============================] - 12s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 23/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0535 - mean_squared_error: 0.0534Epoch 00022: val_loss did not improve\n",
      "23428/23428 [==============================] - 12s - loss: 0.0535 - mean_squared_error: 0.0534 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 24/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00023: val_loss did not improve\n",
      "23428/23428 [==============================] - 12s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 25/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0537 - mean_squared_error: 0.0536Epoch 00024: val_loss did not improve\n",
      "23428/23428 [==============================] - 13s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 26/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00025: val_loss did not improve\n",
      "23428/23428 [==============================] - 14s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 27/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00026: val_loss did not improve\n",
      "23428/23428 [==============================] - 14s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 28/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0534Epoch 00027: val_loss did not improve\n",
      "23428/23428 [==============================] - 14s - loss: 0.0535 - mean_squared_error: 0.0534 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 29/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0535 - mean_squared_error: 0.0534Epoch 00028: val_loss did not improve\n",
      "23428/23428 [==============================] - 13s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 30/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0535 - mean_squared_error: 0.0534Epoch 00029: val_loss did not improve\n",
      "23428/23428 [==============================] - 13s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 31/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00030: val_loss did not improve\n",
      "23428/23428 [==============================] - 14s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 32/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00031: val_loss did not improve\n",
      "23428/23428 [==============================] - 13s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 33/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0537 - mean_squared_error: 0.0536Epoch 00032: val_loss did not improve\n",
      "23428/23428 [==============================] - 14s - loss: 0.0537 - mean_squared_error: 0.0536 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 34/500\n",
      "23296/23428 [============================>.] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0535Epoch 00033: val_loss did not improve\n",
      "23428/23428 [==============================] - 14s - loss: 0.0536 - mean_squared_error: 0.0535 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 00033: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12eee7f90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[118]:\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.io\n",
    "np.random.seed(1337) # for reproducibility\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, AtrousConvolution1D\n",
    "from keras.regularizers import l2, activity_l1\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from seya.layers.recurrent import Bidirectional # after git clone need to python setup.py install again\n",
    "# from keras.utils.layer_utils import print_layer_shapes\n",
    "\n",
    "print 'loading data'\n",
    "# trainmat = h5py.File('/Users/jx/Downloads/deepsea_train/train.mat')\n",
    "validmat = scipy.io.loadmat('/Users/jx/Documents/rbznn/pcavalid.mat')\n",
    "testmat = scipy.io.loadmat('/Users/jx/Documents/rbznn/pcatest.mat')\n",
    "trainmat = scipy.io.loadmat('/Users/jx/Documents/rbznn/pcatrain.mat')\n",
    "\n",
    "X_train = np.expand_dims(np.array(trainmat['tr'][0][0][0]),axis=(1))\n",
    "y_train = np.array(trainmat['tr'][0][0][1]).squeeze()\n",
    "# y_train = y_train.reshape((-1, 1))\n",
    "# y_train.squeeze()\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "\n",
    "\n",
    "\n",
    "lr = 1e-6#learning rate\n",
    "reg = 1e-4\n",
    "print 'building model'\n",
    "nb_filters=32\n",
    "model = Sequential()\n",
    "# model.add(LSTM(32,  W_regularizer=l2(reg),return_sequences=True, input_shape=(4, 113)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(64,  W_regularizer=l2(reg),return_sequences=True)) # return sequences is needed for stacking\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(128,  W_regularizer=l2(reg)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution1D(64, 29, border_mode='same', W_regularizer=l2(reg),input_shape=(1, 47))) # adding conv layer collapses output\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(32, 4, border_mode='same', W_regularizer=l2(reg))) # adding conv layer collapses output\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(32, 4, border_mode='same', W_regularizer=l2(reg))) # adding conv layer collapses output\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(1))\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "\n",
    "print 'compiling model'\n",
    "model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"bestmodel.hdf5\", verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "\n",
    "X_valid=np.expand_dims(validmat['v'][0][0][0],axis=(1))\n",
    "y_valid=np.array(validmat['v'][0][0][1]).squeeze()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=256, nb_epoch=500, shuffle=True, validation_data=(X_valid, y_valid),callbacks=[checkpointer,earlystopper])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "23428/23428 [==============================] - 4s     \n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution1d_28 (Convolution1D) (None, 1, 64)         87296       convolution1d_input_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 1, 64)         0           convolution1d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (None, 1, 64)         0           activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_29 (Convolution1D) (None, 1, 32)         8224        dropout_25[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 1, 32)         0           convolution1d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (None, 1, 32)         0           activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_30 (Convolution1D) (None, 1, 32)         4128        dropout_26[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 1, 32)         0           convolution1d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 1, 32)         0           activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 32)            0           dropout_27[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 100)           3300        flatten_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1)             101         dense_10[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 103049\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "\n",
    "out=model.predict(X_train, batch_size=512,verbose=1)\n",
    "plt.plot(y_train,out,'ro')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "2928/2928 [==============================] - 0s     \n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution1d_28 (Convolution1D) (None, 1, 64)         87296       convolution1d_input_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 1, 64)         0           convolution1d_28[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (None, 1, 64)         0           activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_29 (Convolution1D) (None, 1, 32)         8224        dropout_25[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 1, 32)         0           convolution1d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (None, 1, 32)         0           activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_30 (Convolution1D) (None, 1, 32)         4128        dropout_26[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 1, 32)         0           convolution1d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 1, 32)         0           activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 32)            0           dropout_27[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 100)           3300        flatten_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 1)             101         dense_10[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 103049\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "X_test=np.expand_dims(testmat['tt'][0][0][0],axis=(1))\n",
    "y_test=np.array(testmat['tt'][0][0][1]).squeeze()\n",
    "\n",
    "out=model.predict(X_test, batch_size=512,verbose=1)\n",
    "plt.plot(y_test,out,'ro')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
