{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "'''Trains a LSTM on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import h5py\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import LSTM, SimpleRNN, GRU,Convolution1D,MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-1a05b5c1efc4>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-1a05b5c1efc4>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    print X_train.shape()\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "validmat = scipy.io.loadmat('/Users/jx/Documents/CS273B/validlstm.mat')\n",
    "testmat = scipy.io.loadmat('/Users/jx/Documents/CS273B/testlstm.mat')\n",
    "\n",
    "trainmat = scipy.io.loadmat('/Users/jx/Documents/CS273B/trainlstm.mat')\n",
    "\n",
    "X_train=trainmat['tr'][0][0][0]\n",
    "y_train=trainmat['tr'][0][0][1]\n",
    "\n",
    "X_valid=validmat['v'][0][0][0]\n",
    "y_valid=validmat['v'][0][0][1]\n",
    "\n",
    "X_test=testmat['tt'][0][0][0]\n",
    "y_test=testmat['tt'][0][0][1]\n",
    "\n",
    "print X_train.shape()\n",
    "print y_train.shape()\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "\n",
    "model.add(LSTM(640, dropout_W=0.2, dropout_U=0.2, input_dim=4))  # try using a GRU instead, for fun\n",
    "# model.add(Convolution1D(64, 4, border_mode = 'same',input_shape=(4,111)))\n",
    "# # model.add(Dense(1))\n",
    "# # model.add(Activation('tanh'))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(MaxPooling1D(pool_length=(4)))\n",
    "\n",
    "# model.add(Flatten)\n",
    "model.add(Dense(input_dim=111, output_dim=1))\n",
    "model.add(Activation('softmax'))\n",
    "# model.add(forward_lstm)\n",
    "# model.add(brnn)\n",
    "rmsprop=RMSprop(lr=0.0005, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 8440 samples, validate on 1055 samples\n",
      "Epoch 1/15\n",
      "8320/8440 [============================>.] - ETA: 2s - loss: 7.9597 - acc: 0.5007Epoch 00000: val_loss improved from inf to 8.16008, saving model to bestmodel.hdf5\n",
      "8440/8440 [==============================] - 153s - loss: 7.9466 - acc: 0.5015 - val_loss: 8.1601 - val_acc: 0.4882\n",
      "Epoch 2/15\n",
      "8320/8440 [============================>.] - ETA: 2s - loss: 7.9425 - acc: 0.5018Epoch 00001: val_loss did not improve\n",
      "8440/8440 [==============================] - 156s - loss: 7.9466 - acc: 0.5015 - val_loss: 8.1601 - val_acc: 0.4882\n",
      "Epoch 3/15\n",
      "8320/8440 [============================>.] - ETA: 2s - loss: 7.9539 - acc: 0.5011Epoch 00002: val_loss did not improve\n",
      "8440/8440 [==============================] - 156s - loss: 7.9466 - acc: 0.5015 - val_loss: 8.1601 - val_acc: 0.4882\n",
      "Epoch 4/15\n",
      "8320/8440 [============================>.] - ETA: 2s - loss: 7.9654 - acc: 0.5004Epoch 00003: val_loss did not improve\n",
      "8440/8440 [==============================] - 160s - loss: 7.9466 - acc: 0.5015 - val_loss: 8.1601 - val_acc: 0.4882\n",
      "Epoch 5/15\n",
      "8320/8440 [============================>.] - ETA: 2s - loss: 7.9463 - acc: 0.5016Epoch 00004: val_loss did not improve\n",
      "8440/8440 [==============================] - 153s - loss: 7.9466 - acc: 0.5015 - val_loss: 8.1601 - val_acc: 0.4882\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ece9790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"bestmodel.hdf5\", verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_valid, y_valid), callbacks=[checkpointer,earlystopper])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
